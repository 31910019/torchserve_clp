# 建立服务

torch-model-archiver --model-name BERT4NILM --version 1.0 --serialized-file ap/BERT4NILM.pt --handler ap/NILMHandler.py --export-path ap/ --extra-files ./  --force

torchserve --start --model-store ap/ --models ap/BERT4NILM.mar --ts-config ap/config.properties --ncs

lsof -i :8090 | awk '{ print $2; }' | head -n 2 | grep -v PID | xargs kill -9
lsof -i :8091 | awk '{ print $2; }' | head -n 2 | grep -v PID | xargs kill -9
lsof -i :9000 | awk '{ print $2; }' | head -n 2 | grep -v PID | xargs kill -9
lsof -i :9001 | awk '{ print $2; }' | head -n 2 | grep -v PID | xargs kill -9
docker run --rm -it -p 0.0.0.0:8080:8080 -p 0.0.0.0:8081:8081 -p 0.0.0.0:8082:8082 -v $(pwd)/ap:/home/model-server/model-store pytorch/torchserve:latest-cpu
sleep 2s
curl -X POST "localhost:8081/models?model_name=BERT4NILM&url=BERT4NILM.mar&initial_workers=1"
url2 = "localhost:8081/models?model_name=BERT4NILM&url=BERT4NILM.mar&initial_workers=1"
curl -X POST urls
# requirest测试

import json
import numpy as np
list_data = np.random.randn(1,144).tolist()
# json_data = json.dumps(list_data)

import requests
import json

# 请求数据和 API 路径
data = {"input": list_data[0]}
url = "localhost:8080/predictions/BERT4NILM"
json_data = json.dumps(data)

# 发送 POST 请求到 TorchServe 的 API 路径
response = requests.post(url, json=json_data)

# 处理响应
if response.status_code == 200:
    result = response.json()
    # 处理结果
    print(result)
else:
    print("API request failed with status code:", response.status_code)






